{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting Started with Ollama: Local LLMs for GenAI Apps\n",
        "\n",
        "> **Note:** You need Ollama installed on your machine and its server running on `http://localhost:11434`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1- What is Ollama \n",
        "- Ollama is a lightweight runtime that serves open‑source LLMs locally (CPU or GPU) via a simple HTTP API on `localhost:11434`.\n",
        "- visit https://ollama.com/search to check all the available models\n",
        "#### a- Install Ollama\n",
        "**Download**: check this page and download the version for your OS: https://ollama.com/download\n",
        "\n",
        "After installation, start the server:\n",
        "\n",
        "```bash\n",
        "ollama serve\n",
        "```\n",
        "\n",
        "Check version:\n",
        "```bash\n",
        "ollama --version\n",
        "```\n",
        "\n",
        "To bind on a custom host/port:\n",
        "```bash\n",
        "OLLAMA_HOST=0.0.0.0:11434 ollama serve\n",
        "```\n",
        "\n",
        "#### b- Pull a model\n",
        "Run this **in a terminal** (first run downloads weights):\n",
        "\n",
        "```bash\n",
        "ollama pull llama3.1\n",
        "```\n",
        "\n",
        "You can also try `mistral`, `qwen2.5`, or a vision model like `llama3.2-vision`. Models available in https://ollama.com/search\n",
        "#### c- Check and run the model in terminal\n",
        "Run this **in a terminal**:\n",
        "\n",
        "```bash\n",
        "ollama list\n",
        "```\n",
        "the if the model is downloaded:\n",
        "\n",
        "```bash\n",
        "ollama run llama3.1\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2) Verify the server is reachable\n",
        "We ping the local API and display available models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json, requests\n",
        "from pprint import pprint\n",
        "\n",
        "BASE_URL = 'http://localhost:11434'\n",
        "\n",
        "def check_server(base_url=BASE_URL):\n",
        "    try:\n",
        "        r = requests.get(base_url)\n",
        "        return r.status_code, r.text[:200]\n",
        "    except Exception as e:\n",
        "        return None, str(e)\n",
        "\n",
        "status, info = check_server()\n",
        "print('Server status:', status)\n",
        "print('Info (truncated):', info)\n",
        "\n",
        "def list_models(base_url=BASE_URL):\n",
        "    try:\n",
        "        r = requests.get(base_url + '/api/tags', timeout=10)\n",
        "        r.raise_for_status()\n",
        "        return r.json()\n",
        "    except Exception as e:\n",
        "        return {'error': str(e)}\n",
        "\n",
        "pprint(list_models())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3) Python client install\n",
        "Install the official Python package that talks to the local Ollama API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If needed, uncomment and run:\n",
        "# !pip install ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Simple chat completion (non‑streaming)\n",
        "This uses the `ollama` client. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    from ollama import Client\n",
        "    client = Client(host=BASE_URL)\n",
        "    resp = client.chat(\n",
        "        model='llama3.2:1b',\n",
        "        messages=[\n",
        "            {'role': 'system', 'content': 'You are a concise teaching assistant.'},\n",
        "            {'role': 'user', 'content': 'Explain GANs in 4 bullet points.'}\n",
        "        ],\n",
        "        options={'temperature': 0.7, 'num_ctx': 4096}\n",
        "    )\n",
        "    print(resp['message']['content'])\n",
        "except Exception as e:\n",
        "    print('⚠️ Chat call failed. Ensure the server is running and the model is pulled. Error:\\n', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Streaming tokens (nice for demos)\n",
        "The server can stream partial responses. This is useful in apps to show incremental output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    from ollama import Client\n",
        "    client = Client(host=BASE_URL)\n",
        "    stream = client.chat(\n",
        "        model='llama3.2:1b',\n",
        "        messages=[{'role': 'user', 'content': 'List 5 use cases of RAG.'}],\n",
        "        stream=True,\n",
        "    )\n",
        "    for chunk in stream:\n",
        "        print(chunk['message']['content'], end='', flush=True)\n",
        "    print()\n",
        "except Exception as e:\n",
        "    print('⚠️ Streaming call failed. Error:\\n', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Embeddings for mini‑RAG\n",
        "We generate an embedding vector for a short text. You can index multiple texts and do cosine similarity to retrieve relevant passages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    from ollama import Client\n",
        "    import numpy as np\n",
        "    client = Client(host=BASE_URL)\n",
        "    text = 'Diffusion models iteratively denoise data to sample from complex distributions.'\n",
        "    emb = client.embeddings(model='llama3.2:1b', prompt=text)\n",
        "    vec = np.array(emb['embedding'])\n",
        "    print('Embedding length:', len(vec))\n",
        "    print('First 8 dims:', vec[:8])\n",
        "except Exception as e:\n",
        "    print('⚠️ Embedding call failed. Error:\\n', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Raw REST with `requests` (non‑streaming)\n",
        "Sometimes you want to see the plain HTTP calls for debugging or to avoid extra dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json, requests\n",
        "payload = {\n",
        "  'model': 'llama3.2:1b',\n",
        "  'messages': [\n",
        "    {'role': 'system', 'content': 'Be brief.'},\n",
        "    {'role': 'user', 'content': 'What is a VAE, in 3 sentences max?'}\n",
        "  ],\n",
        "  'options': {'temperature': 0.6},\n",
        "  'stream': False\n",
        "}\n",
        "try:\n",
        "    r = requests.post(BASE_URL + '/api/chat', json=payload, timeout=120)\n",
        "    r.raise_for_status()\n",
        "    print(r.json()['message']['content'])\n",
        "except Exception as e:\n",
        "    print('⚠️ REST call failed. Error:\\n', e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "558284ab",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
